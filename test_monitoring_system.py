#!/usr/bin/env python3
"""
Comprehensive test suite for the FinBrief monitoring and observability system.
Tests logging, metrics collection, health checks, and API endpoints.
"""
import sys
import os
import time
import asyncio
import tempfile
from datetime import datetime
from pathlib import Path

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))


def test_logging_system():
    """Test the advanced logging system"""
    print("üìù Testing Logging System")
    print("=" * 40)
    
    try:
        from src.monitoring.logger import get_logger, log_crawler_activity, log_api_request
        
        # Test basic logger functionality
        logger = get_logger("test")
        logger.info("Test info message", test_param="value", number=42)
        logger.debug("Test debug message", debug_data={"key": "value"})
        logger.warning("Test warning message", warning_level="medium")
        
        # Test error logging
        try:
            raise ValueError("Test exception")
        except ValueError as e:
            logger.error("Test error occurred", exception=e, context="testing")
        
        print("‚úÖ Basic logging functionality works")
        
        # Test performance monitoring
        with logger.performance_timer("test_operation"):
            time.sleep(0.1)
        
        print("‚úÖ Performance timing works")
        
        # Test component-specific logging
        log_crawler_activity("Test crawler activity", source="test_source", count=10)
        log_api_request("Test API request", method="GET", endpoint="/test", status_code=200)
        
        print("‚úÖ Component-specific logging works")
        
        # Check if log files were created
        log_dir = Path("logs")
        if log_dir.exists():
            log_files = list(log_dir.glob("*.log"))
            print(f"‚úÖ Created {len(log_files)} log files")
        else:
            print("‚ö†Ô∏è  Log directory not found")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Logging system test failed: {e}")
        return False


def test_metrics_system():
    """Test the metrics collection system"""
    print("\nüìä Testing Metrics System")
    print("=" * 40)
    
    try:
        from src.monitoring.metrics import get_metrics, FinBriefMetrics
        
        # Test basic metrics functionality
        metrics = get_metrics()
        
        # Test counter metrics
        metrics.record_news_crawled("test_source", 25)
        counter_value = metrics.collector.get_counter_value("news_articles_crawled_total", source="test_source")
        print(f"‚úÖ Counter metrics work (value: {counter_value})")
        
        # Test timer metrics
        metrics.record_crawl_duration("test_source", 2.5)
        timer_stats = metrics.collector.get_timer_stats("crawler_duration_seconds", source="test_source")
        print(f"‚úÖ Timer metrics work (count: {timer_stats.get('count', 0)})")
        
        # Test gauge metrics
        metrics.record_memory_usage("test_component", 256.5)
        gauge_value = metrics.collector.get_gauge_value("memory_usage_mb", component="test_component")
        print(f"‚úÖ Gauge metrics work (value: {gauge_value})")
        
        # Test histogram metrics
        metrics.record_nlp_processing("test_processor", "sentiment", 0.8, 500)
        hist_stats = metrics.collector.get_histogram_stats("nlp_text_length_characters", processor="test_processor", operation="sentiment")
        print(f"‚úÖ Histogram metrics work (count: {hist_stats.get('count', 0)})")
        
        # Test API metrics
        metrics.record_api_request("/api/test", "GET", 200, 0.15)
        metrics.record_api_request("/api/test", "GET", 200, 0.12)
        api_stats = metrics.collector.get_timer_stats("api_request_duration_seconds", endpoint="/api/test", method="GET")
        print(f"‚úÖ API metrics work (avg: {api_stats.get('avg', 0):.4f}s)")
        
        # Test database metrics
        metrics.record_database_operation("SELECT", "test_table", 0.05, 10)
        db_stats = metrics.collector.get_timer_stats("database_operation_duration_seconds", operation="SELECT", table="test_table")
        print(f"‚úÖ Database metrics work (count: {db_stats.get('count', 0)})")
        
        # Test metrics summary
        summary = metrics.get_metrics_summary()
        print(f"‚úÖ Metrics summary generated (components: {len(summary)})")
        
        # Test metrics persistence
        metrics.collector._flush_metrics()
        metrics_dir = Path("logs/metrics")
        if metrics_dir.exists():
            metric_files = list(metrics_dir.glob("metrics_*.json"))
            print(f"‚úÖ Metrics persisted ({len(metric_files)} files)")
        else:
            print("‚ö†Ô∏è  Metrics directory not found")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Metrics system test failed: {e}")
        return False


async def test_health_system():
    """Test the health monitoring system"""
    print("\nüè• Testing Health System")
    print("=" * 40)
    
    try:
        from src.monitoring.health import get_health_monitor, check_system_health, check_all_health
        
        # Test basic health monitor
        monitor = get_health_monitor()
        print(f"‚úÖ Health monitor created with {len(monitor.health_checkers)} checks")
        
        # Test individual health checks
        result = await monitor.run_single_check("system_resources")
        if result:
            print(f"‚úÖ System resources check: {result.status.value} ({result.duration_ms:.1f}ms)")
        
        result = await monitor.run_single_check("disk_space")
        if result:
            print(f"‚úÖ Disk space check: {result.status.value} ({result.duration_ms:.1f}ms)")
        
        result = await monitor.run_single_check("memory")
        if result:
            print(f"‚úÖ Memory check: {result.status.value} ({result.duration_ms:.1f}ms)")
        
        # Test database check (may fail if not configured)
        result = await monitor.run_single_check("database")
        if result:
            print(f"‚úÖ Database check: {result.status.value} ({result.duration_ms:.1f}ms)")
        
        # Test vector store check (may be degraded if dependencies missing)
        result = await monitor.run_single_check("vector_store")
        if result:
            print(f"‚úÖ Vector store check: {result.status.value}")
        
        # Test all health checks
        all_results = await monitor.run_all_checks()
        print(f"‚úÖ All health checks completed ({len(all_results)} checks)")
        
        # Test system health summary
        summary = monitor.get_system_health_summary()
        print(f"‚úÖ System health summary: {summary['status']} (uptime: {summary['uptime_seconds']:.1f}s)")
        
        # Test convenience functions
        quick_health = await check_system_health()
        print(f"‚úÖ Quick health check: {quick_health['status']}")
        
        detailed_health = await check_all_health()
        print(f"‚úÖ Detailed health check completed")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Health system test failed: {e}")
        return False


def test_monitoring_api():
    """Test the monitoring API endpoints"""
    print("\nüåê Testing Monitoring API")
    print("=" * 40)
    
    try:
        from src.api.monitoring_api import router
        
        # Check API router creation
        routes = [route.path for route in router.routes]
        expected_endpoints = [
            "/monitoring/health",
            "/monitoring/health/detailed",
            "/monitoring/health/{component}",
            "/monitoring/metrics/summary",
            "/monitoring/metrics/raw",
            "/monitoring/metrics/performance",
            "/monitoring/logs",
            "/monitoring/status",
            "/monitoring/ping"
        ]
        
        endpoints_found = 0
        for expected in expected_endpoints:
            if any(expected.replace("{component}", "test") in route or expected == route for route in routes):
                endpoints_found += 1
        
        print(f"‚úÖ API endpoints: {endpoints_found}/{len(expected_endpoints)} defined")
        
        # Test API models
        from src.api.monitoring_api import SystemHealthResponse, MetricsSummaryResponse, HealthCheckResponse
        
        # Test model creation
        health_response = SystemHealthResponse(
            status="healthy",
            message="System is healthy",
            uptime_seconds=3600.0,
            timestamp=datetime.now(),
            checks={"total": 5, "healthy": 5},
            details={}
        )
        
        print("‚úÖ API response models work")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Monitoring API test failed: {e}")
        return False


def test_monitoring_middleware():
    """Test the monitoring middleware"""
    print("\nüîß Testing Monitoring Middleware")
    print("=" * 40)
    
    try:
        from src.monitoring.middleware import MonitoringMiddleware, monitor_operation, PerformanceTracker, monitor_function
        
        # Test middleware creation
        from fastapi import FastAPI
        app = FastAPI()
        middleware = MonitoringMiddleware(app)
        print("‚úÖ Monitoring middleware created")
        
        # Test performance tracker
        tracker = PerformanceTracker()
        tracker.start_operation("test_op", "test_operation", component="test")
        time.sleep(0.1)
        tracker.end_operation("test_op", success=True)
        print("‚úÖ Performance tracker works")
        
        # Test function decorator
        @monitor_function("test_function", component="test")
        def test_sync_function():
            time.sleep(0.05)
            return "test result"
        
        result = test_sync_function()
        print(f"‚úÖ Function monitoring decorator works: {result}")
        
        print("‚úÖ Monitoring middleware components work")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Monitoring middleware test failed: {e}")
        return False


async def test_operation_monitoring():
    """Test operation monitoring context manager"""
    print("\n‚ö° Testing Operation Monitoring")
    print("=" * 40)
    
    try:
        from src.monitoring.middleware import monitor_operation
        
        # Test successful operation
        async with monitor_operation("test_success_operation", component="test", param="value"):
            await asyncio.sleep(0.1)
            print("‚úÖ Successful operation monitored")
        
        # Test failed operation
        try:
            async with monitor_operation("test_failed_operation", component="test"):
                await asyncio.sleep(0.05)
                raise ValueError("Test error")
        except ValueError:
            print("‚úÖ Failed operation monitored")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Operation monitoring test failed: {e}")
        return False


def test_integration():
    """Test integration between monitoring components"""
    print("\nüîó Testing Integration")
    print("=" * 40)
    
    try:
        from src.monitoring.logger import get_logger
        from src.monitoring.metrics import get_metrics
        from src.monitoring.health import get_health_monitor
        
        # Test that components work together
        logger = get_logger("integration_test")
        metrics = get_metrics()
        health_monitor = get_health_monitor()
        
        # Simulate a workflow
        logger.info("Starting integration test workflow")
        
        # Record some metrics
        metrics.record_news_crawled("integration_test", 15)
        metrics.record_api_request("/api/integration", "POST", 201, 0.25)
        
        # Log the metrics
        logger.info("Metrics recorded", 
                   articles_crawled=15, 
                   api_response_time=0.25)
        
        # Check system health would be affected
        logger.info("Integration test completed successfully")
        
        print("‚úÖ Component integration works")
        return True
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        return False


async def main():
    """Run comprehensive monitoring system tests"""
    print("üîç FinBrief Monitoring & Observability System Test")
    print("=" * 70)
    print(f"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    tests = [
        ("Logging System", test_logging_system),
        ("Metrics System", test_metrics_system),
        ("Health System", test_health_system),
        ("Monitoring API", test_monitoring_api),
        ("Monitoring Middleware", test_monitoring_middleware),
        ("Operation Monitoring", test_operation_monitoring),
        ("Integration", test_integration)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"--- {test_name} ---")
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            
            if result:
                passed += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                print(f"‚ùå {test_name} FAILED")
        except Exception as e:
            print(f"‚ùå {test_name} FAILED: {e}")
        
        print()
    
    print("=" * 70)
    print(f"üìä Results: {passed}/{total} tests passed")
    
    if passed >= total * 0.8:  # 80% pass rate
        print("üéâ Monitoring & Observability system ready!")
        print()
        print("üìã Features Available:")
        print("‚úÖ Structured JSON logging with rotation")
        print("‚úÖ Comprehensive metrics collection (counters, gauges, timers, histograms)")
        print("‚úÖ System health monitoring (8+ health checks)")
        print("‚úÖ REST API endpoints for monitoring data")
        print("‚úÖ Automatic request/response tracking middleware")
        print("‚úÖ Database operation monitoring")
        print("‚úÖ Performance tracking and alerting")
        print("‚úÖ Error tracking and debugging support")
        
        print("\nüöÄ Deployment Guide:")
        print("1. Logs are stored in: logs/ directory")
        print("2. Metrics are stored in: logs/metrics/ directory")
        print("3. API endpoints available at: /monitoring/*")
        print("4. Health checks at: /monitoring/health")
        print("5. Quick status at: /monitoring/ping")
        
        print("\nüí° Usage Examples:")
        print("- Check system health: GET /monitoring/health")
        print("- View metrics: GET /monitoring/metrics/summary")
        print("- Monitor performance: GET /monitoring/metrics/performance")
        print("- View logs: GET /monitoring/logs (admin only)")
        print("- Test monitoring: POST /monitoring/test (admin only)")
        
    else:
        print("‚ö†Ô∏è  Some monitoring features need attention")
        print("üí° Check error messages above and install missing dependencies")
    
    # Clean up test files
    try:
        metrics = get_metrics()
        metrics.collector.stop()
    except:
        pass
    
    return passed >= total * 0.5


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)